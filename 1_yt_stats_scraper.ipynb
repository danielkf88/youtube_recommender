{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and packages \n",
    "from pytube import YouTube\n",
    "from pytube import exceptions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To get all links from a channel please follow instruction from the link\n",
    "https://softexpert.pk/how-to-copy-all-the-titles-and-urls-from-youtube-channel/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is to scroll the page so as to load all video\n",
    "var scroll = setInterval(function(){ window.scrollBy(0, 1000)}, 1000);\n",
    "\n",
    "# This is to stop scroll plus retrieve link for all regular youtube video links\n",
    "window.clearInterval(scroll); console.clear(); \n",
    "urls = $$('a'); urls.forEach(function(v,i,a){if (v.id==\"video-title-link\")\n",
    "{console.log('\\t'+v.title+'\\t'+v.href+'\\t')}});\n",
    "\n",
    "# This is to stop scroll plus retrieve link for all shorts youtube video links\n",
    "window.clearInterval(scroll); \n",
    "console.clear(); \n",
    "shorts = $$('ytd-rich-item-renderer');\n",
    "shorts.forEach(function(short, i, shorts) {\n",
    "  const title = short.querySelector('#video-title').textContent;\n",
    "  const url = short.querySelector('#thumbnail').href;\n",
    "  console.log('\\t'+title+'\\t'+url+'\\t');\n",
    "});"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating all csv files gotten from above steps before scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file names of links in csv of individual youtube channel\n",
    "files = ['alex.csv', 'assemblyai.csv', 'coding_is_fun.csv', 'data_prof.csv', 'giant_neural_network.csv', \n",
    "         'jkim.csv', 'josh_brindley.csv', 'ken_jee.csv', 'lukas.csv', 'luke_barousse.csv', \n",
    "         'nicholas_renotte.csv', 'olle_green.csv', 'rob_mulla.csv', 'shashank.csv', 'statquest.csv', \n",
    "         'sundas.csv','thu_vu.csv', 'tina_huang.csv', 'turp.csv', 'underfit.csv',\n",
    "         'steve_brunton.csv', 'smitha_kolan.csv', 'mukul_rathi.csv', 'ayush_singh.csv', 'hebbar.csv',\n",
    "         'daniel_bourke.csv', 'liam_ottley.csv', 'jash_radia.csv', 'seattle_data.csv', 'zach.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all file to a single csv\n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    df = pd.concat([df, pd.read_csv(f\"data/channel_link/{file}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export a copy for record and for 2_yt_caption.ipynb\n",
    "df.to_csv('data/vid_link_final.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vid_link_final.csv can be found in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/vid_link_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of yt_stats_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new list to attribute of each video \n",
    "author = []\n",
    "title = []\n",
    "publish_date = []\n",
    "views = []\n",
    "likes = []\n",
    "length = []\n",
    "url = []\n",
    "types = []\n",
    "keywords = []\n",
    "image = []\n",
    "description = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regex template to extract like count \n",
    "like_template = r'[0-9]{1,3},?[0-9]{0,3},?[0-9]{0,3} like'\n",
    "\n",
    "# read vid_link.csv prepare from earlier steps\n",
    "df = pd.read_csv('data/vid_link_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please note that this will have a very very long run time due to youtube anti scraping\n",
    "### Final file from this notebook can be found in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loop through the list of unique id\n",
    "for link in df['link']:\n",
    "    \n",
    "    yt = YouTube(link)\n",
    "    \n",
    "    try:\n",
    "        # try if there is error for get the attribute\n",
    "        yt.title\n",
    "\n",
    "    except exceptions.PytubeError:\n",
    "        # if error, wait and try again\n",
    "        print('  error at', link, len(title))\n",
    "        time.sleep(np.random.randint(60, 120))\n",
    "        yt = YouTube(link)\n",
    "\n",
    "        try:\n",
    "            # 2nd try block needed to catch occassionally back to back error\n",
    "            yt.title\n",
    "        except exceptions.PytubeError:\n",
    "            print('  error2 at', link, len(title))\n",
    "            time.sleep(np.random.randint(80, 180))\n",
    "            yt = YouTube(link)\n",
    "            str_like = re.search(like_template, str(yt.initial_data)).group(0)\n",
    "            like = int(str_like.split(' ')[0].replace(',', ''))\n",
    "            author.append(yt.author)\n",
    "            title.append(yt.title)\n",
    "            publish_date.append(yt.publish_date)\n",
    "            views.append(yt.views)\n",
    "            likes.append(like)\n",
    "            length.append(yt.length)\n",
    "            url.append(link)\n",
    "            types.append((link).split('/')[3])\n",
    "            keywords.append(yt.keywords)\n",
    "            image.append(yt.thumbnail_url)\n",
    "            description.append(yt.description)\n",
    "            \n",
    "            time.sleep(np.random.randint(10, 20))\n",
    "        else:\n",
    "            str_like = re.search(like_template, str(yt.initial_data)).group(0)\n",
    "            like = int(str_like.split(' ')[0].replace(',', ''))\n",
    "            author.append(yt.author)\n",
    "            title.append(yt.title)\n",
    "            publish_date.append(yt.publish_date)\n",
    "            views.append(yt.views)\n",
    "            likes.append(like)\n",
    "            length.append(yt.length)\n",
    "            url.append(link)\n",
    "            types.append((link).split('/')[3])\n",
    "            keywords.append(yt.keywords)\n",
    "            image.append(yt.thumbnail_url)\n",
    "            description.append(yt.description)\n",
    "            print(len(title), end='\\r')\n",
    "            time.sleep(np.random.randint(10, 20))\n",
    "            df2 = pd.DataFrame({'author': author, \n",
    "                                'title': title,\n",
    "                                'publish_date': publish_date,\n",
    "                                'views': views,\n",
    "                                'likes': likes,\n",
    "                                'url': url,\n",
    "                                'types': types,\n",
    "                                'length': length,\n",
    "                                'keywords': keywords,\n",
    "                                'image': image,\n",
    "                                'description': description})\n",
    "            df2.to_csv(f'vid_info2_{len(author)}.csv', index=False)\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        # if no error, append the attribute to the list\n",
    "        try:\n",
    "            re.search(like_template, str(yt.initial_data)).group(0)\n",
    "        except AttributeError:\n",
    "            print(' initial_data error at', link, len(title))\n",
    "            time.sleep(np.random.randint(60, 120))\n",
    "            yt = YouTube(link)\n",
    "            str_like = re.search(like_template, str(yt.initial_data)).group(0)\n",
    "            like = int(str_like.split(' ')[0].replace(',', ''))\n",
    "            author.append(yt.author)\n",
    "            title.append(yt.title)\n",
    "            publish_date.append(yt.publish_date)\n",
    "            views.append(yt.views)\n",
    "            likes.append(like)\n",
    "            length.append(yt.length)\n",
    "            url.append(link)\n",
    "            types.append((link).split('/')[3])\n",
    "            keywords.append(yt.keywords)\n",
    "            image.append(yt.thumbnail_url)\n",
    "            description.append(yt.description)\n",
    "            print(len(title), end='\\r')\n",
    "            time.sleep(np.random.randint(10, 20))\n",
    "        else:\n",
    "            str_like = re.search(like_template, str(yt.initial_data)).group(0)\n",
    "            like = int(str_like.split(' ')[0].replace(',', ''))\n",
    "            author.append(yt.author)\n",
    "            title.append(yt.title)\n",
    "            publish_date.append(yt.publish_date)\n",
    "            views.append(yt.views)\n",
    "            likes.append(like)\n",
    "            length.append(yt.length)\n",
    "            url.append(link)\n",
    "            types.append((link).split('/')[3])\n",
    "            keywords.append(yt.keywords)\n",
    "            image.append(yt.thumbnail_url)\n",
    "            description.append(yt.description)\n",
    "            print(len(title), end='\\r')\n",
    "            time.sleep(np.random.randint(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe with the extracted attributes\n",
    "df2 = pd.DataFrame({'author': author, \n",
    "                    'title': title,\n",
    "                    'publish_date': publish_date,\n",
    "                    'views': views,\n",
    "                    'likes': likes,\n",
    "                    'url': url,\n",
    "                    'types': types,\n",
    "                    'length': length,\n",
    "                    'keywords': keywords,\n",
    "                    'image': image,\n",
    "                    'description': description})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('data/vid_info_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
